{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for painter image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from helpers.preprocessing_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "origin_dir = \"data/raw\"\n",
    "new_dir = \"data/preprocessed\"\n",
    "#painters = [\"Mondriaan\", \"Picasso\", \"Rubens\"]\n",
    "painters = [\"Picasso\", \"Rubens\"]\n",
    "\n",
    "im = Image.open(f\"{origin_dir}/{'Picasso'}/0-picassofull-1.JPG\")\n",
    "# im.show()\n",
    "im.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_list = []\n",
    "# valid_images = [\".jpg\",\".png\",\".jpeg\"]\n",
    "# painter = painters[1]\n",
    "# orig_path = f\"{origin_dir}/{painter}/\"\n",
    "# new_path = f\"{new_dir}/{painter}/\"\n",
    "\n",
    "# for f in os.listdir(orig_path):\n",
    "#     ext = os.path.splitext(f)[1]\n",
    "#     if ext.lower() not in valid_images:\n",
    "#         print(f)\n",
    "#         continue\n",
    "#     #print(f\"saving {f} to {new_path}\")\n",
    "#     img = Image.open(os.path.join(orig_path,f))\n",
    "#     cover = img.resize((180, 180))\n",
    "#     #cover.save(os.path.join(new_path,f))\n",
    "# # print(type(image_list))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    list = get_raw_file_name_for_painter(\"no\")\n",
    "    list\n",
    "except FileNotFoundError:\n",
    "    print(\"correct error caught\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = get_raw_file_name_for_painter(painters[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make_data_sets(painters, 600) \n",
    "# write an fuction that loop over an list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [1, 2, 3, 6, 5, 6, 7, 8, 9]\n",
    "print(list[6:9], list[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "800//6*4+800//6*1+800//6*1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = \"test.png\"\n",
    "pre, _ = os.path.splitext(demo)\n",
    "#os.rename(renamee, pre + new_extension)\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "new_base_dir = \"data/preprocessed/\"\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    os.path.join(new_base_dir, \"train\"),\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    os.path.join(new_base_dir, \"val\"),\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    os.path.join(new_base_dir, \"test\"),\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_dataset.class_names))\n",
    "print(type(validation_dataset))\n",
    "print(type(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from PIL import Image\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "PATH_TO_SAVE = \"data/raw/Rembrandt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir(dir:str):\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "def download_image(pic_url:str, saveName:str):\n",
    "    with open(saveName, 'wb') as handle:\n",
    "        response = requests.get(pic_url, stream=True)\n",
    "        if not response.ok:\n",
    "            pass\n",
    "        for block in response.iter_content(1024):\n",
    "            if not block:\n",
    "                break\n",
    "            handle.write(block)\n",
    "\n",
    "def download_image_from_site_default(uri:str):\n",
    "    r = requests.get(uri)\n",
    "    s = BeautifulSoup(r.text, \"html.parser\")\n",
    "    part = s.find(id=\"workimage\")\n",
    "    src = part.find(\"img\")['src']\n",
    "    pic_url = uri[:uri.rfind(\"/\")] + \"/\" + src\n",
    "    saveName = PATH_TO_SAVE +pic_url[pic_url.rfind(\"/\"):]\n",
    "    # if os.path.exists(saveName):\n",
    "    #     name, ext = os.path.splitext(saveName)\n",
    "    #     name += str(uuid.uuid4())\n",
    "    #     saveName = name + ext\n",
    "    download_image(pic_url, saveName)\n",
    "\n",
    "def download_image_from_site_try_2(uri:str):\n",
    "    r = requests.get(uri)\n",
    "    s = BeautifulSoup(r.text, \"html.parser\")\n",
    "    images = s.find_all(\"img\")\n",
    "    images = str(images)\n",
    "    \n",
    "    start = images.find('src=\"images/')\n",
    "    images = images[start+5:]\n",
    "    end = images.find('\"')\n",
    "    src = images[:end]\n",
    "    \n",
    "    pic_url = uri[:uri.rfind(\"/\")] + \"/\" + src\n",
    "    saveName = PATH_TO_SAVE +pic_url[pic_url.rfind(\"/\"):]\n",
    "    # if os.path.exists(saveName):\n",
    "    #     name, ext = os.path.splitext(saveName)\n",
    "    #     name += str(uuid.uuid4())\n",
    "    #     saveName = name + ext\n",
    "    download_image(pic_url, saveName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_menu_items(uri:str):\n",
    "    links = []\n",
    "    r = requests.get(uri)\n",
    "    s = BeautifulSoup(r.text, \"html.parser\")\n",
    "    #  Looking for the table with the classes 'wikitable' and 'sortable'\n",
    "    table = s.find('table', class_='tablelinks')\n",
    "    i =0\n",
    "    if table == None:\n",
    "        return None\n",
    "\n",
    "    for row in table.find_all('tr'): \n",
    "        columns = row.find_all('td')\n",
    "        td = columns[0]\n",
    "        td = str(td)\n",
    "        start = td.find('href=\"')\n",
    "        td = td[start+6:]\n",
    "        end = td.find('\"')\n",
    "        img_page_link = td[:end]\n",
    "        if img_page_link != 0: \n",
    "            i+=1\n",
    "            links.append(img_page_link)\n",
    "    return links\n",
    "\n",
    "def get_table(uri:str):# Creating list with all tables\n",
    "    links = []\n",
    "    r = requests.get(uri)\n",
    "    s = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    #  Looking for the table with the classes 'wikitable' and 'sortable'\n",
    "    table = s.find('table', class_='tablelinks')\n",
    "    i =0\n",
    "    if table == None:\n",
    "        return None\n",
    "\n",
    "    for row in table.find_all('tr'): \n",
    "        columns = row.find_all('td')\n",
    "        td = columns[1]\n",
    "        td = str(td)\n",
    "        start = td.find('href=\"')\n",
    "        td = td[start+6:]\n",
    "        end = td.find('\"')\n",
    "        img_page_link = td[:end]\n",
    "        if img_page_link != 0: \n",
    "            i+=1\n",
    "            links.append(img_page_link)\n",
    "        # print(img_page_link)\n",
    "\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://www.rembrandtpainting.net/\"\n",
    "start_uri = \"http://www.rembrandtpainting.net/complete_catalogue/complete_catalogue.htm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_items = get_menu_items(start_uri)\n",
    "menu_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in menu_items:\n",
    "    new_base = \"http://www.rembrandtpainting.net/complete_catalogue/\"\n",
    "    uri = new_base+page\n",
    "\n",
    "    table = get_table(uri)\n",
    "    check_dir(PATH_TO_SAVE)\n",
    "    done = []\n",
    "    for i in table:\n",
    "        if new_base+i in done:\n",
    "            pass\n",
    "        else:\n",
    "            done.append(new_base+i)\n",
    "            try:\n",
    "                download_image_from_site_default(new_base+i)\n",
    "            except:\n",
    "                try: \n",
    "                    download_image_from_site_try_2(new_base+i)\n",
    "                except:\n",
    "                    print(f\"error at: {new_base+i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(PATH_TO_SAVE):\n",
    "    try:\n",
    "        img = Image.open(PATH_TO_SAVE+\"/\"+filename) # open the image file\n",
    "        img.verify() # verify that it is, in fact an image\n",
    "        img.close()\n",
    "    except (IOError, SyntaxError) as e:\n",
    "        print('Bad file:', filename) # print out the names of corrupt files\n",
    "        os.remove(PATH_TO_SAVE+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"aantal images: {len(os.listdir(PATH_TO_SAVE))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Activation\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs=15\n",
    "image_shape = (180, 180)\n",
    "PREPROCESSING_FOLDER = \"data/preprocessed\"\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                f\"{PREPROCESSING_FOLDER}/train\", image_size=image_shape, batch_size=batch_size, label_mode='categorical'\n",
    "                )\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                f\"{PREPROCESSING_FOLDER}/test\", image_size=image_shape, batch_size=batch_size, label_mode='categorical'\n",
    "                )\n",
    "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                f\"{PREPROCESSING_FOLDER}/val\", image_size=image_shape, batch_size=batch_size, label_mode='categorical'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape = len(train_dataset.class_names)\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(output_shape, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"rmsprop\",metrics=[\"accuracy\"])\n",
    "\n",
    "filename = \"test.keras\"\n",
    "callback = [keras.callbacks.ModelCheckpoint(filename, save_best_only=True)]\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset, type(train_dataset))\n",
    "print(val_dataset, type(val_dataset))\n",
    "print(batch_size, type(batch_size))\n",
    "print(epochs, type(epochs))\n",
    "print(callback, type(callback))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, epochs=epochs, \n",
    "        validation_data=(val_dataset), \n",
    "        callbacks=callback\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 180  # figure size vergroten\n",
    "\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "ax1.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "ax1.set_title(\"Accuracy\")\n",
    "ax1.legend();\n",
    "\n",
    "ax2.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "ax2.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "ax2.set_title(\"Loss\")\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Activation\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "image_shape = (180, 180)\n",
    "PREPROCESSING_FOLDER = \"data/preprocessed\"\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                f\"{PREPROCESSING_FOLDER}/test\", image_size=image_shape, batch_size=batch_size, label_mode='categorical'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (180, 180, 3)\n",
    "output_shape = 3\n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(0.1),\n",
    "            layers.RandomZoom(0.2),\n",
    "            ])\n",
    "        \n",
    "conv_base = keras.applications.vgg19.VGG19(\n",
    "            weights=\"imagenet\",\n",
    "            include_top=False\n",
    "            )\n",
    "\n",
    "conv_base.trainable = True\n",
    "for layer in conv_base.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "x = inputs\n",
    "x = data_augmentation(x) \n",
    "\n",
    "x = keras.applications.vgg19.preprocess_input(x)\n",
    "x = conv_base(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(output_shape, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"rmsprop\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"painter_baseline.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"zeeuws_meisje_piet_mondriaan.jpg\"\n",
    "filename, _ = os.path.splitext(f'static/preprocessed/{filename}')\n",
    "\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                f\"{PREPROCESSING_FOLDER}/test\", image_size=image_shape, batch_size=batch_size, label_mode='categorical'\n",
    "                )\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open(filename + \".png\")\n",
    "\n",
    "numpydata = np.asarray(img)\n",
    "numpydata = np.expand_dims(numpydata, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.utils.load_img(\n",
    "    filename + \".png\", target_size=(180, 180)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = model.predict(numpydata)\n",
    "predictions = model.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "class_names = test_dataset.class_names\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "painters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b474f1d58789c1fe159b53a18b6fa0b19156323cd2c932ec0b19aac0b35ab2b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
