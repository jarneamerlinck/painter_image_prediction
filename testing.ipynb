{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from PIL import Image\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "PATH_TO_SAVE = \"data/raw/Rembrandt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir(dir:str):\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "def download_image(pic_url:str, saveName:str):\n",
    "    with open(saveName, 'wb') as handle:\n",
    "        response = requests.get(pic_url, stream=True)\n",
    "        if not response.ok:\n",
    "            pass\n",
    "        for block in response.iter_content(1024):\n",
    "            if not block:\n",
    "                break\n",
    "            handle.write(block)\n",
    "\n",
    "def download_image_from_site_default(uri:str):\n",
    "    r = requests.get(uri)\n",
    "    s = BeautifulSoup(r.text, \"html.parser\")\n",
    "    part = s.find(id=\"workimage\")\n",
    "    src = part.find(\"img\")['src']\n",
    "    pic_url = uri[:uri.rfind(\"/\")] + \"/\" + src\n",
    "    saveName = PATH_TO_SAVE +pic_url[pic_url.rfind(\"/\"):]\n",
    "    # if os.path.exists(saveName):\n",
    "    #     name, ext = os.path.splitext(saveName)\n",
    "    #     name += str(uuid.uuid4())\n",
    "    #     saveName = name + ext\n",
    "    download_image(pic_url, saveName)\n",
    "\n",
    "def download_image_from_site_try_2(uri:str):\n",
    "    r = requests.get(uri)\n",
    "    s = BeautifulSoup(r.text, \"html.parser\")\n",
    "    images = s.find_all(\"img\")\n",
    "    images = str(images)\n",
    "    \n",
    "    start = images.find('src=\"images/')\n",
    "    images = images[start+5:]\n",
    "    end = images.find('\"')\n",
    "    src = images[:end]\n",
    "    \n",
    "    pic_url = uri[:uri.rfind(\"/\")] + \"/\" + src\n",
    "    saveName = PATH_TO_SAVE +pic_url[pic_url.rfind(\"/\"):]\n",
    "    # if os.path.exists(saveName):\n",
    "    #     name, ext = os.path.splitext(saveName)\n",
    "    #     name += str(uuid.uuid4())\n",
    "    #     saveName = name + ext\n",
    "    download_image(pic_url, saveName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_menu_items(uri:str):\n",
    "    links = []\n",
    "    r = requests.get(uri)\n",
    "    s = BeautifulSoup(r.text, \"html.parser\")\n",
    "    #  Looking for the table with the classes 'wikitable' and 'sortable'\n",
    "    table = s.find('table', class_='tablelinks')\n",
    "    i =0\n",
    "    if table == None:\n",
    "        return None\n",
    "\n",
    "    for row in table.find_all('tr'): \n",
    "        columns = row.find_all('td')\n",
    "        td = columns[0]\n",
    "        td = str(td)\n",
    "        start = td.find('href=\"')\n",
    "        td = td[start+6:]\n",
    "        end = td.find('\"')\n",
    "        img_page_link = td[:end]\n",
    "        if img_page_link != 0: \n",
    "            i+=1\n",
    "            links.append(img_page_link)\n",
    "    return links\n",
    "\n",
    "def get_table(uri:str):# Creating list with all tables\n",
    "    links = []\n",
    "    r = requests.get(uri)\n",
    "    s = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    #  Looking for the table with the classes 'wikitable' and 'sortable'\n",
    "    table = s.find('table', class_='tablelinks')\n",
    "    i =0\n",
    "    if table == None:\n",
    "        return None\n",
    "\n",
    "    for row in table.find_all('tr'): \n",
    "        columns = row.find_all('td')\n",
    "        td = columns[1]\n",
    "        td = str(td)\n",
    "        start = td.find('href=\"')\n",
    "        td = td[start+6:]\n",
    "        end = td.find('\"')\n",
    "        img_page_link = td[:end]\n",
    "        if img_page_link != 0: \n",
    "            i+=1\n",
    "            links.append(img_page_link)\n",
    "        # print(img_page_link)\n",
    "\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://www.rembrandtpainting.net/\"\n",
    "start_uri = \"http://www.rembrandtpainting.net/complete_catalogue/complete_catalogue.htm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_items = get_menu_items(start_uri)\n",
    "menu_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in menu_items:\n",
    "    new_base = \"http://www.rembrandtpainting.net/complete_catalogue/\"\n",
    "    uri = new_base+page\n",
    "\n",
    "    table = get_table(uri)\n",
    "    check_dir(PATH_TO_SAVE)\n",
    "    done = []\n",
    "    for i in table:\n",
    "        if new_base+i in done:\n",
    "            pass\n",
    "        else:\n",
    "            done.append(new_base+i)\n",
    "            try:\n",
    "                download_image_from_site_default(new_base+i)\n",
    "            except:\n",
    "                try: \n",
    "                    download_image_from_site_try_2(new_base+i)\n",
    "                except:\n",
    "                    print(f\"error at: {new_base+i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(PATH_TO_SAVE):\n",
    "    try:\n",
    "        img = Image.open(PATH_TO_SAVE+\"/\"+filename) # open the image file\n",
    "        img.verify() # verify that it is, in fact an image\n",
    "        img.close()\n",
    "    except (IOError, SyntaxError) as e:\n",
    "        print('Bad file:', filename) # print out the names of corrupt files\n",
    "        os.remove(PATH_TO_SAVE+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"aantal images: {len(os.listdir(PATH_TO_SAVE))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Activation\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "image_shape = (180, 180)\n",
    "PREPROCESSING_FOLDER = \"data/preprocessed\"\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                f\"{PREPROCESSING_FOLDER}/test\", image_size=image_shape, batch_size=batch_size, label_mode='categorical'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (180, 180, 3)\n",
    "output_shape = 3\n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(0.1),\n",
    "            layers.RandomZoom(0.2),\n",
    "            ])\n",
    "        \n",
    "conv_base = keras.applications.vgg19.VGG19(\n",
    "            weights=\"imagenet\",\n",
    "            include_top=False\n",
    "            )\n",
    "\n",
    "conv_base.trainable = True\n",
    "for layer in conv_base.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "x = inputs\n",
    "x = data_augmentation(x) \n",
    "\n",
    "x = keras.applications.vgg19.preprocess_input(x)\n",
    "x = conv_base(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(output_shape, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"rmsprop\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"painter_baseline.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"zeeuws_meisje_piet_mondriaan.jpg\"\n",
    "filename, _ = os.path.splitext(f'static/preprocessed/{filename}')\n",
    "\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                f\"{PREPROCESSING_FOLDER}/test\", image_size=image_shape, batch_size=batch_size, label_mode='categorical'\n",
    "                )\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open(filename + \".png\")\n",
    "\n",
    "numpydata = np.asarray(img)\n",
    "numpydata = np.expand_dims(numpydata, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.utils.load_img(\n",
    "    filename + \".png\", target_size=(180, 180)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = model.predict(numpydata)\n",
    "predictions = model.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "class_names = test_dataset.class_names\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "painters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b474f1d58789c1fe159b53a18b6fa0b19156323cd2c932ec0b19aac0b35ab2b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
